<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=author content="map[]"><title>Past Events</title><link rel="shortcut icon" href=https://munich-nlp.github.io/images/favicon.png><link href=https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.no-icons.min.css rel=stylesheet><script defer src=https://use.fontawesome.com/releases/v5.0.11/js/all.js integrity=sha384-ImVoB8Er8knetgQakxuBS4G3RSkyD8IZVVQCAnmRJrDwqJFYUE4YOv+DbIofcO9C crossorigin=anonymous></script>
<link rel=stylesheet href="https://fonts.googleapis.com/css?family=Alice|Open+Sans:400,300,700"><link rel=stylesheet href=https://munich-nlp.github.io/css/styles.min.ee9878fb7975c98855fa270ff7cfab08fc1644cf609d7aa8cb8338f5cba9dd1f2fafaf1e708aab938e9e61081d30e63175872b714e7a4dee0c40f128687f4075.css integrity="sha512-7ph4+3l1yYhV+icP98+rCPwWRM9gnXqoy4M49cup3R8vr68ecIqrk46eYQgdMOYxdYcrcU56Te4MQPEoaH9AdQ=="><style>@import 'https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@300&display=swap'</style></head><body class=home><header id=header><div id=head class=parallax data-parallax-speed=2 style=background-image:url(https://munich-nlp.github.io/)><h1 id=logo class=text-center><img class=img-noborder src=https://munich-nlp.github.io/images/munichnlp.png alt>
<span class=tagline><br><a href=mailto:></a></span></h1></div><nav class="navbar navbar-default"><div class=container-fluid><div class=navbar-header><button type=button class=navbar-toggle data-toggle=collapse data-target=#bs-example-navbar-collapse-1 aria-expanded=true>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button></div><div class="navbar-collapse collapse" id=bs-example-navbar-collapse-1><ul class="nav navbar-nav"><li><a href=/#about>About</a></li><li><a href=/#upcomingevents>Upcoming events</a></li><li><a href=/#pastevents>Past events</a></li><li><a href=/#partners>Partners</a></li><li><a href=/#organizers>Organizers</a></li></ul></div></div></nav></header><main id=main><div class=container><div class="row topspace"><div class="col-sm-8 col-sm-offset-2"><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>February 2, 2023</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/better-and-faster-nlp-model-training/ rel=bookmark>Better and Faster NLP Model Training: 10 Proven Techniques</a></h1></header><div class=entry-content><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/UbL1QMwDpec style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=about-this-event>About this Event</h3><p>In this talk, we will explore the latest and most effective techniques for training natural language processing models. We will cover methods that have been proven to work in Kaggle competitions, such as AWP, SWA, and pseudolabeling, as well as best practices learned from W&amp;B power users. With these techniques, you&rsquo;ll be able to train your models faster and achieve better performance. Whether you&rsquo;re a seasoned machine learning engineer or just getting started, you&rsquo;ll come away with valuable insights and practical tips for boosting your NLP model training.</p><h3 id=speakers>Speakers</h3><p><img src="https://media.licdn.com/dms/image/C4E03AQGYMUWZr1RIXw/profile-displayphoto-shrink_200_200/0/1600865897838?e=1679529600&amp;v=beta&amp;t=aKlV5WJGKyOa91m4whHaHKcNqHuoLCkFLqfL2anNM4Y" alt="Darek KÅ‚eczek ><"></p><p><a href=https://de.linkedin.com/in/achimliese><strong>Darek KÅ‚eczek</strong></a> is a Machine Learning Engineer at Weights & Biases where he is leading the W&amp;B education program. Previously, he applied machine learning across
supply chain, manufacturing, legal and commercial use cases. He also worked on operationalizing machine learning at P&amp;G. Darek has contributed the first Polish versions of BERT and GPT language models and has been a leader in the Polish NLP community. Heâ€™s a Kaggle competition</p></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>December 22, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/ekin-akyurek-in-context-learning/ rel=bookmark>What learning algorithm is in-context learning?</a></h1></header><div class=entry-content><p><img src=/images/ev_nlp_in_con_len_ekin_akyurek/theme_photo.jpg alt="What learning algorithm is in-context learning? ><"></p><h3 id=about-this-event>About this event</h3><p>Ekin AkyÃ¼rek from MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) will explain his recent work including</p><ul><li>The hypothesis that language models learn in-context by learning algorithms running internally.</li><li>Novel way of proving that the Transformer architecture can learn in-context by internal gradient descent and ridge regression without any approximation to the architecture</li><li>The empirical fact that the trained Transformers learn in-context very close to Bayes optimal algorithms, and do not learn in-context by gradient descent.</li><li>Phase shifts between algorithmic behavior and scaling laws based model size vs problem dimension</li><li>The distinction between behavior and computation.</li></ul><h3 id=speaker>Speaker</h3><p><img src=https://www.ekinakyurek.me/assets/ekin.jpg alt="Ekin AkyÃ¼rek ><"></p><p><strong>Ekin AkyÃ¼rek</strong> is a computer science PhD student at the Massachusetts Institute of Technology (MIT). He studies artificial intelligence through natural language processing and machine learning and is advised by Jacob Andreas.</p><p>He works on improving sequence modeling for language processing and understanding.</p></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>December 15, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/11th-nlp-meetup/ rel=bookmark>11th NLP Meetup</a></h1></header><div class=entry-content><p><img src=/images/nlpmeetup.png alt="11th NLP Meetup ><"></p><h3 id=about-this-event>About this event</h3><p>We would like to invite you to the 11th NLP meetup with two very interesting talks and a great Xmas special surprise ðŸ˜Š Thanks to our host Munich Re, we will again meet at a very cool location in the middle of Munich. ðŸš€</p><p>In the first talk, we will learn about NLP activities @ Munich Re and what impact large language models (LLMs) could potentially have on use cases in the finance and insurance domain. The second talk discusses a very interesting study on sequential multi-task learning comparing training language models on a smaller set of related tasks vs. the large-scale multi-task training with potential to reduce computational resources and costs. So, let us getting inspired by great ideas and insights by these two talks ðŸ˜Š</p><p>Eventually, we want to thank Munich Re for hosting this evening and for organizing a very special Xmas surprise for our NLP community.</p><h3 id=speakers>Speakers</h3><p><img src=/images/oliver-pfaffel.jpg alt="Oliver Pfaffel ><"></p><p><strong>Dr. Oliver Pfaffel</strong> is an AI data scientist at Munich Re.</p><p><img src=/images/matthias-kaper.jpg alt="Matthias Kaper ><"></p><p><strong>Dr. Matthias Kaper</strong> is the Lead NLP at Munich Re.</p><p><img src=/images/muhtasham-oblokulov.jpg alt="Muhtasham Oblokulov ><"></p><p><strong>Muhtasham Oblokulov</strong> is an ML engineer at Munich Re and the co-founder of Munich NLP.</p></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>December 8, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/pydata-munichnlp-vol2/ rel=bookmark>PyData x MunichNLP Vol. 2</a></h1></header><div class=entry-content><p><img src=/images/ev_nlp_pydata_v2/theme_photo_2.png alt="PyData x MunichNLP Vol. 2 ><"></p><h3 id=about-this-event>About this event</h3><p>We are back with our second collaborative event with PyData (Vol. 2)! The event will be held in-person at JetBrains Munich, where we will be providing drinks and pizza. Register for the event <a href="https://www.meetup.com/pydata-munchen/events/289404388/?_xtd=gqFyqTMyNjU0NTQxOKFwo2FwaQ%253D%253D&amp;from=ref">here</a>.</p><p>We have exciting talks from two amazing speakers:</p><ol><li><p><strong>Reproducible ML Workflows & Dev Environments with dstack</strong> by <strong>Andrey Cheptsov, dstack</strong></p><p>Building ML models is an iterative process. Letâ€™s talk about tools and practices that help set up your dev environments and workflows for better productivity and reproducibility?</p><p>As a bonus, weâ€™ll have an overview of dstack, an open-source utility that simplifies the MLOps stack, and helps run ML workflows and dev environments in the cloud.</p><p><strong>Speaker</strong>
<img src=/images/andrey-cheptsov.jpeg alt="Andrey Cheptsov ><"></p><p><strong>Andrey Cheptsov</strong> is the creator of dstack. He is passionate about open-source and developer tools for AI. Previously, Andrey worked at JetBrains with the PyCharm team.</p></li><li><p><strong>Why ML Should be Written as Pipelines from the Get-Go</strong> by <strong>Hamza Tahir, ZenML</strong></p><p>The mechanism through which ML propagates through an organisation from experimentation to production is key to its success. Oftentimes, there is a tendency to break this mechanism into a multi-step process, where experimentation workflows are siloed from their production counter-parts. This &ldquo;Throw it over the wall&rdquo; anti-pattern can stunt the velocity of ML teams. In this talk, we talk about why teams should unify this multi-stage process, and give data scientists more agency to exercise control over their production workflows. We&rsquo;ll also go through a practical demonstration with creating a unified MLOps pipeline with ZenML.</p><p><strong>Speaker</strong>
<img src=/images/hamza-tahir.jpg alt="Hamza Tahir ><"></p><p><strong>Hamza Tahir</strong> is a software developer turned ML engineer. An indie hacker by heart, he loves ideating, implementing, and launching data-driven products. His previous projects include PicHance, Scrilys, BudgetML, and you-tldr. Based on his learnings from deploying ML in production for predictive maintenance use-cases in his previous startup, he co-created ZenML, an open-source MLOps framework to build portable production-ready ML pipelines.</p></li></ol></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>November 23, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/open-source-teaching-resources-for-nlp/ rel=bookmark>Open-Source Teaching Resources for NLP</a></h1></header><div class=entry-content><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/HBP-JJ6xEQc style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=about-this-event>About this Event</h3><p>Key technologies, like e.g. NLP, crucially depend on well-educated people to conduct sound and innovative research. While the quality and the availability of teaching resources plays a central role here, it is still not super common to (collaboratively) create and share them across institutions. In this talk, I will cover three main aspects:
First, the development of open source educational resources (OSER) within the scope of the i2ml lecture at the SLDS chair (LMU Munich) will be showcased. Second, I will cover the latest developments in NLP in order to show the necessity of creating high quality OSER for this field to equip undergrads / young researchers with the methodological background to eventually perform good research. Third, I will briefly introduce a joint project between two chairs from LMU (SLDS, CIS) and Ben Roth&rsquo;s chair (UNIVIE) with the goal to create OSER for NLP following the blueprint of i2ml.</p><p>The presented course can be accessed <a href=https://slds-lmu.github.io/dl4nlp/>here</a>.</p><h3 id=speaker>Speaker</h3><p><img src=/images/assenmacher.png alt="Matthias AÃŸenmacher ><"></p><p><strong>Dr. Matthias AÃŸenmacher</strong> is a postdoctoral researcher at the Chair of Statistical Learning and Data Science (Dept. of Statistics, LMU) and the NFDI Consortium for Business, Economic and Related Data (BERD@NFDI). He obtained his Bachelorâ€™s degree in Economics from LMU in 2014, afterwards he turned to Statistics (with a focus on social and economic studies) and obtained his Masterâ€™s degree in 2017 (also from LMU). In October 2021, he finished his PhD at the working group Methods for Missing Data, Model Selection and Model Averaging under the supervision of Prof. Dr. Christian Heumann with a focus on Natural Language Processing. Further, he is one of the co-founders of the <a href=https://www.statistik.uni-muenchen.de/institut/osis/index.html>Open Science Initiative in Statistics (OSIS)</a>.</p></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>November 22, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/challenges-and-opportunities-in-nlp-for-under-represented-languages/ rel=bookmark>Challenges and Opportunities in NLP for Under-Represented Languages</a></h1></header><div class=entry-content><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/uk5ygedi1fQ style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=about-this-event>About this event</h3><p>Natural language processing (NLP) technology has seen tremendous improvements in recent years but most of these successes have been concentrated in languages with large amounts of data. In this talk, I will discuss challenges and potential solutions on the way to scaling NLP to more of the world&rsquo;s 7000 languages. In particular, I will highlight recent progress in NLP for African languages and present methods that are applicable to languages with limited data such as employing alternative sources of data and multi-modal information.</p><h3 id=speaker>Speaker</h3><p><img src=https://ruder.io/content/images/2019/02/new_profile_photo_square-1.jpg alt="Sebastian Ruder ><"></p><p><strong>Sebastian Ruder</strong> is a research scientist at Google based in Berlin, Germany working on natural language processing (NLP) for under-represented languages. Before that he was a research scientist at DeepMind. He completed his PhD in Natural Language Processing and Deep Learning at the Insight Research Centre for Data Analytics, while working as a research scientist at Dublin-based text analytics startup AYLIEN.</p></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>November 17, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/pydata-munichnlp-vol1/ rel=bookmark>PyData x MunichNLP Vol. 1</a></h1></header><div class=entry-content><p><img src=/images/ev_nlp_pydata_v1/theme_photo.png alt="PyData x MunichNLP Vol. 1 ><"></p><h3 id=location>Location</h3><p>JetBrains Munich. Register for the event <a href="https://www.meetup.com/pydata-munchen/events/289404125/?_xtd=gqFyqTMyNjU0NTQxOKFwo2FwaQ%253D%253D&amp;from=ref">here</a>.</p><h3 id=about-this-event>About this event</h3><p>We are proud to present to you our first collaborative event with PyData! The event will be held in-person at JetBrains Munich, where we will be providing drinks and pizza. Register for the event <a href="https://www.meetup.com/pydata-munchen/events/289404125/?_xtd=gqFyqTMyNjU0NTQxOKFwo2FwaQ%253D%253D&amp;from=ref">here</a>.</p><p>We have exciting talks from two amazing speakers:</p><ol><li><p><strong>Practical Text Search or how to beat Elasticsearch (when you have to)</strong> by <strong>Egor Labintcev, Alyne GmbH</strong></p><p>In this talk, Egor is going to guide you through the process of building a good text search engine when itâ€™s hard to do so. Heâ€™ll start by defining what actually makes the text search good (itâ€™s not that trivial), continue with a colourful description of the struggle the speaker went through while building a (relatively) good text search, and culminate by stating the best practices and approaches.</p><p><strong>Speaker</strong>
<img src=/images/egor-labintcev.jpg alt="Egor Labintcev ><"></p><p><strong>Egor Labintcev</strong> is currently a Senior ML Engineer in the RegTech industry. He builds various text engines ranging from low-resource extreme multi-label classification to ensemble-based text search in complex domains. His current research interest lies in modern retrieval approaches and NLP model interpretability.</p></li><li><p><strong>Fraud Detection with Graph Features and GNN</strong> by <strong>Nikita Iserson, S&amp;P Global</strong></p><p>Identifying fraudulent behaviors is becoming increasingly more complex as technology advances and fraudsters constantly evolve new ways to exploit people, companies, and institutions. The complexity grows as companies introduce new channels, platforms, and devices for customers to engage with their brand, manage their accounts, and make transactions. Graph neural networks (GNN) are increasingly being used to identify suspicious behavior. GNNs can combine graph structures, such as email accounts, addresses, phone numbers, and purchasing behavior to find meaningful patterns and enhance fraud detection.</p><p><strong>Speaker</strong>
<img src=/images/nikita-iserson.jpg alt="Nikita Iserson ><"></p><p><strong>Nikita Iserson</strong> is a Lead Machine Learning Engineer at S&amp;P Global with over 10 years of experience in software engineering, data warehouse development, data analytics, and machine learning. He has built demand forecasting, network analysis, recommender systems, digital twins, and much more covering a wide range of industries, including telecom, retail, and banking.</p></li></ol></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>November 10, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/tricks-and-tools-from-nlp-land/ rel=bookmark>Tricks and Tools from NLP Land</a></h1></header><div class=entry-content><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/sjiASMMbHao style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=about-this-event>About this event</h3><p>What if we did not use transformers? Can we still do NLP? Yes.</p><p>In this talk, Vincent will discuss some effective tricks in NLP that don&rsquo;t require a transformer or a GPU. It will be a collection of techniques that he has seen work in the field including some tricks with embeddings, annotation techniques, tf-idf, subword embeddings, and multi-word embeddings. Many of these techniques can also be used outside of the realm of NLP and most of the topics will be discussed via a live demo.</p><p>If there is time, Vincent might also show a very cool fine-tuning trick!</p><h3 id=speaker>Speaker</h3><p><img src=/images/vincent-warmerdam.jpg alt="Vincent Warmerdam ><"></p><p><strong>Vincent Warmerdam</strong> worked as an engineer, consultant, researcher, team lead, and educator in the past. He currently works as a Machine Learning Engineer at Explosion. He also maintains many small open source packages, including many scikit-learn related plugins. He blogs over at koaning.io and he also maintains an increasingly popular free learning resource over at calmcode.io</p></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>October 26, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/nlp-for-social-good-survey/ rel=bookmark>NLP for Social Good: A Survey with Use-Cases</a></h1></header><div class=entry-content><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/B-LBo-w4CrA style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=about-this-event>About this event</h3><p>Today, a large spectrum of NLP models have been developed covering various fields ranging from already-usual-for-everyday web search, to helpers for programmers to write code. However, quite a few of the modern NLP technologies were explored in terms of their application for social good.</p><h3 id=speaker>Speaker</h3><p><img src=/images/ev_nlp_nlp4socg_daryna_dem/daryna-dementieva.png alt="Daryna Dementieva ><"></p><p><strong>Daryna Dementieva</strong> is a post-doc researcher at Social Research Computing Lab in TUM. Previously, she finished her PhD in Skoltech University under the supervision of Prof. Alexander Panchenko with the topic &ldquo;Methods for Fighting Harmful Multilingual Textual Content&rdquo;. In general, Daryna&rsquo;s research is connected with different applications of NLP for Social Good. During her PhD, she worked with topics of fake news detection and text detoxification. Moreover, she participated in several competitions and hackathons wrapping obtained raw NLP models into product demonstrations. Daryna would like to inspire with her studies research and development of NLP-based products for real-life applications with positive impact.</p></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>October 13, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/simplifying-mlops-stack/ rel=bookmark>Simplifying MLOps Stack</a></h1></header><div class=entry-content><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/vdDdX-xf_mM style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=about-this-event>About this event</h3><p>Running ML workflows involves a lot of hurdles, especially if you donâ€™t have a FAANG-level ML infrastructure. Classical MLOps platforms provide a certain level of automation but often require a complex setup and may stand in the way more than help. Particularly, if you want to run modular code (rather than spread it over a notebook), use Git, and your favourite code editor and terminal. Join this meet-up to learn more about the tool, and general best practices related to setting up your dev environment for ML.</p><h3 id=speaker>Speaker</h3><p><img src=/images/andrey-cheptsov.jpeg alt="Andrey Cheptsov ><"></p><p><strong>Andrey Cheptsov</strong> is the creator of dstack, a very lightweight open-source utility that allows to provision ML infra via command-line. He is passionate about open-source and developer tools for AI. Previously, Andrey worked at JetBrains with the PyCharm team.</p></div></article></div></div><center class><ul class=pagination><ul class="pagination pagination-default"><li class=page-item><a href=/home/pastevents/ aria-label=First class=page-link role=button><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/home/pastevents/page/2/ aria-label=Previous class=page-link role=button><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a href=/home/pastevents/ aria-label="Page 1" class=page-link role=button>1</a></li><li class=page-item><a href=/home/pastevents/page/2/ aria-label="Page 2" class=page-link role=button>2</a></li><li class="page-item active"><a aria-current=page aria-label="Page 3" class=page-link role=button>3</a></li><li class=page-item><a href=/home/pastevents/page/4/ aria-label="Page 4" class=page-link role=button>4</a></li><li class=page-item><a href=/home/pastevents/page/4/ aria-label=Next class=page-link role=button><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/home/pastevents/page/4/ aria-label=Last class=page-link role=button><span aria-hidden=true>&#187;&#187;</span></a></li></ul></ul></center></div></main><footer id=footer><div class=container><div class=row><div class="col-md-3 widget"><h3 class=widget-title>Follow us!</h3><div class=widget-body><p class=follow-me-icons><a href=https://discord.com/invite/BgFaZgZ38N target=_blank><i class="fab fa-discord fa-1x"></i></a>
<a href=https://www.youtube.com/channel/UCmpRQbcw7dOiXSyWpGYwk3Q target=_blank><i class="fab fa-youtube fa-1x"></i></a>
<a href=https://twitter.com/munichnlp target=_blank><i class="fab fa-twitter-square fa-1x"></i></a>
<a href=https://www.linkedin.com/company/munich-nlp/ target=_blank><i class="fab fa-linkedin fa-1x"></i></a>
<a href=https://github.com/Munich-NLP target=_blank><i class="fab fa-github fa-1x"></i></a>
<a href=mailto:munichnlp@gmail.com target=_blank><i class="fas fa-envelope-square fa-1x"></i></a></p></div></div></div></div></footer><script src=https://code.jquery.com/jquery-1.12.4.min.js></script>
<script src=https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js></script>
<script src=https://munich-nlp.github.io/js/bundle.min.8adc00f11fb328590a42ed3b14d3ec5b116abb98395a8a662d782f2150a7de6cc85eacb26c14724bb0c5130a11be2933c9b4f835cd3a053e90f036e210fbdd5d.js integrity="sha512-itwA8R+zKFkKQu07FNPsWxFqu5g5WopmLXgvIVCn3mzIXqyybBRyS7DFEwoRvikzybT4Nc06BT6Q8DbiEPvdXQ=="></script>
<script id=dsq-count-scr src=//hugo-initio-site.disqus.com/count.js async></script>
<script></script></body></html></body></html>