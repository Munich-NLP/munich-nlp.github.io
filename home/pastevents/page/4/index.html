<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=author content="map[]"><title>Past Events</title><link rel="shortcut icon" href=https://munich-nlp.github.io/images/favicon.png><link href=https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.no-icons.min.css rel=stylesheet><script defer src=https://use.fontawesome.com/releases/v5.0.11/js/all.js integrity=sha384-ImVoB8Er8knetgQakxuBS4G3RSkyD8IZVVQCAnmRJrDwqJFYUE4YOv+DbIofcO9C crossorigin=anonymous></script>
<link rel=stylesheet href="https://fonts.googleapis.com/css?family=Alice|Open+Sans:400,300,700"><link rel=stylesheet href=https://munich-nlp.github.io/css/styles.min.ee9878fb7975c98855fa270ff7cfab08fc1644cf609d7aa8cb8338f5cba9dd1f2fafaf1e708aab938e9e61081d30e63175872b714e7a4dee0c40f128687f4075.css integrity="sha512-7ph4+3l1yYhV+icP98+rCPwWRM9gnXqoy4M49cup3R8vr68ecIqrk46eYQgdMOYxdYcrcU56Te4MQPEoaH9AdQ=="><style>@import 'https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@300&display=swap'</style></head><body class=home><header id=header><div id=head class=parallax data-parallax-speed=2 style=background-image:url(https://munich-nlp.github.io/)><h1 id=logo class=text-center><img class=img-noborder src=https://munich-nlp.github.io/images/munichnlp.png alt>
<span class=tagline><br><a href=mailto:></a></span></h1></div><nav class="navbar navbar-default"><div class=container-fluid><div class=navbar-header><button type=button class=navbar-toggle data-toggle=collapse data-target=#bs-example-navbar-collapse-1 aria-expanded=true>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button></div><div class="navbar-collapse collapse" id=bs-example-navbar-collapse-1><ul class="nav navbar-nav"><li><a href=/#about>About</a></li><li><a href=/#upcomingevents>Upcoming events</a></li><li><a href=/#pastevents>Past events</a></li><li><a href=/#partners>Partners</a></li><li><a href=/#organizers>Organizers</a></li></ul></div></div></nav></header><main id=main><div class=container><div class="row topspace"><div class="col-sm-8 col-sm-offset-2"><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>October 13, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/transformers-in-all-glory-details/ rel=bookmark>Transformers in All Glory Details</a></h1></header><div class=entry-content><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/dqb4U-QzMbs style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=about-this-event>About this event</h3><p>In this talk, Lucas will take a deep dive into the Transformer architecture and the attention mechanism. He will provide context around the model, and explain how it has recently been adapted to various ML communities/modalities. The slides are available at <a href=http://lucasb.eyer.be/transformer>http://lucasb.eyer.be/transformer</a></p><h3 id=speaker>Speaker</h3><p><img src=/images/lucas-beyer.jpeg alt="Lucas Beyer ><"></p><p><strong>Lucas Beyer</strong> grew up in Belgium wanting to make video games and their AI, went on to study mechanical engineering at RWTH Aachen in Germany, did a PhD in robotic perception/computer vision there too, and is now researching representation learning and vision backbones at Google Brain in ZÃ¼rich.</p></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>September 22, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/next-generation-of-semantic-search/ rel=bookmark>Next Generation of Semantic Search</a></h1></header><div class=entry-content><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/Y0g_ENURKFk style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=about-this-event>About this event</h3><p>Using deep neural networks to map text to dense vector spaces (also known as semantic search), has brought tremendous progress to textual information retrieval. However, recent research showed that this widely adopted approach is extremely sensitive to data drift and performs poorly on out-of-domain and long tail queries.</p><p>In this talk, Nils will give an introduction to the next generation of semantic search architectures. First, he will talk about hybrid continuous-binary approaches that can lead up to a 100 times cost reduction for deploying semantic search. Then, he will present learned sparse representations that perform a lot better in terms of data drift, out-of-domain and long tail queries. These approaches are especially suited in domains with quickly evolving information needs like news retrieval.</p><h3 id=speaker>Speaker</h3><p><img src=/images/nils-reimers.jpeg alt="Nils Reimers ><"></p><p><strong>Nils Reimers</strong> is an expert on search relevance using pre-trained transformer networks. In 2018, he authored and open-sourced the popular sentence-transformers.</p></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>July 6, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/language-models-for-symbolic-music-generation/ rel=bookmark>Language Models for Symbolic Music Generation</a></h1></header><div class=entry-content><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/LZQIQxogFEk style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=about-this-event>About this event</h3><p>Tristan Behrens will present how Language Models can be used to compose music, thus effectively bridging the GAP between NLP and symbolic music generation, and further talk about his experience in the field.</p><h3 id=speaker>Speaker</h3><p><img src=/images/tristan-behrens.jpeg alt="Tristan Behrens ><"></p><p><strong>Tristan Behrens</strong> is an expert in AI, an AI composer, and an AI educator. He has an extensive track record in successful Deep Learning projects. His biggest focus is Deep Neural Networks for composition. He has published several albums of music composed with the computer.</p></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>June 29, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/accelerating-transformers-in-production/ rel=bookmark>Accelerating Transformers in Production</a></h1></header><div class=entry-content><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/CAbHbm9769Q style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=about-this-event>About this event</h3><p>Lewis Tunstall will talk about optimization of transformer models. He will cover knowledge distillation and weight quantization as well as frameworks like ONNX Runtime and Hugging Face Optimum.</p><h3 id=speaker>Speaker</h3><p><img src=/images/lewis-tunstall.jpeg alt="Lewis Tunstall ><"></p><p><strong>Lewis Tunstall</strong> is a Machine Learning Engineer at Hugging Face. He is responsible for implementing the tooling to conduct large-scale evaluations of the 10,000+ models and 1,000+ datasets hosted on the Hugging Face Hub. Recently, he published a book called &ldquo;Natural Language Processing with Transformers&rdquo;.</p></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>June 23, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/hands-on-tutorial-huggingface-gradio/ rel=bookmark>A Hands-on Tutorial of the Hugging Face Hub and Gradio</a></h1></header><div class=entry-content><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/EazTFBSpdns style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=about-this-event>About this event</h3><p>Abubakar Abid will hold an online tutorial covering practical tools for modern Machine Learning, ML datasets, models, and demos. He will present how to use the HuggingFace Hub, an online playground for ML models, quickly find suitable models and datasets for your ML tasks, and how to make them work using Gradio.</p><h3 id=speaker>Speaker</h3><p><img src=/images/abubakar-abid.jpeg alt="Abubakar Abid ><"></p><p><strong>Abubakar Abid</strong> is the founder of <a href=https://www.gradio.dev>Gradio</a> and currently ML Team Lead at HuggingFace. He holds a PhD from Stanford University, in which his research focus was building reliable deep learning models with applications to biology and medicine.</p></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>June 18, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/pretrained-language-models-evaluating-themselves/ rel=bookmark>Pre-trained language models evaluating themselves - A comparative study</a></h1></header><div class=entry-content><h3 id=about-this-event>About this event</h3><p>We will take a dive into the field of evaluation of generated text where Philipp Koch will present his paper &ldquo;Pre-trained language models evaluating themselves - A comparative study&rdquo; (<a href=https://aclanthology.org/2022.insights-1.25/)>https://aclanthology.org/2022.insights-1.25/)</a>, which was recently accepted to the Workshop on Insights from Negative Results in NLP @ACL'22.</p><p>Evaluation of generated text remains a significant issue. Recently introduced model-based metrics have shown promising results compared to n-gram-based metrics like BLEU, yet they still suffer severe drawbacks.</p><h3 id=speaker>Speaker</h3><p><img src=/images/ev_nlp_langmod_eval_phil_koch/philipp-koch.jpg alt="Philipp Koch ><"></p><p><strong>Philipp Koch</strong> is one of the founders of Munich NLP. He is currently studying M.Sc. Statistics and Data Science at LMU.</p></div></article><article class=post><header class=entry-header><div class=entry-meta><span class=posted-on><time class="entry-date published" datetime>May 27, 2022</time></span></div><h1 class=entry-title><a href=https://munich-nlp.github.io/events/life-after-bert/ rel=bookmark>Life after BERT: What do Other Muppets Understand about Language?</a></h1></header><div class=entry-content><h3 id=about-this-event>About this event</h3><p>How does model architecture, pre-training objective, the side of the dataset and parameter count affect model&rsquo;s linguistic abilities? They don&rsquo;t ð¤¯. Or at least not as directly we thought.
Evaluation of generated text remains a significant issue. Recently-introduced model-based metrics have shown promising results compared to n-gram-based metrics like BLEU, yet they still suffer severe drawbacks (<a href=http://arxiv.org/abs/2205.10696)>http://arxiv.org/abs/2205.10696)</a>.</p><h3 id=speaker>Speaker</h3><p><img src=/images/vlad.jpeg alt="Vladislav Lialin ><"></p><p><strong>Vladislav Lialin</strong> is a computer science PhD student at University of Massachusetts Lowell advised by Anna Rumshisky. His research areas include continual learning for large language models, multimodal learning, and model analysis. In particular, he is hyping large-scale models and thinks that every task is a language modeling task if you try hard enough. He is currently interning at Amazon Alexa AI.</p></div></article></div></div><center class><ul class=pagination><ul class="pagination pagination-default"><li class=page-item><a href=/home/pastevents/ aria-label=First class=page-link role=button><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/home/pastevents/page/3/ aria-label=Previous class=page-link role=button><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a href=/home/pastevents/ aria-label="Page 1" class=page-link role=button>1</a></li><li class=page-item><a href=/home/pastevents/page/2/ aria-label="Page 2" class=page-link role=button>2</a></li><li class=page-item><a href=/home/pastevents/page/3/ aria-label="Page 3" class=page-link role=button>3</a></li><li class="page-item active"><a aria-current=page aria-label="Page 4" class=page-link role=button>4</a></li><li class="page-item disabled"><a aria-disabled=true aria-label=Next class=page-link role=button tabindex=-1><span aria-hidden=true>&#187;</span></a></li><li class="page-item disabled"><a aria-disabled=true aria-label=Last class=page-link role=button tabindex=-1><span aria-hidden=true>&#187;&#187;</span></a></li></ul></ul></center></div></main><footer id=footer><div class=container><div class=row><div class="col-md-3 widget"><h3 class=widget-title>Follow us!</h3><div class=widget-body><p class=follow-me-icons><a href=https://discord.com/invite/BgFaZgZ38N target=_blank><i class="fab fa-discord fa-1x"></i></a>
<a href=https://www.youtube.com/channel/UCmpRQbcw7dOiXSyWpGYwk3Q target=_blank><i class="fab fa-youtube fa-1x"></i></a>
<a href=https://twitter.com/munichnlp target=_blank><i class="fab fa-twitter-square fa-1x"></i></a>
<a href=https://www.linkedin.com/company/munich-nlp/ target=_blank><i class="fab fa-linkedin fa-1x"></i></a>
<a href=https://github.com/Munich-NLP target=_blank><i class="fab fa-github fa-1x"></i></a>
<a href=mailto:munichnlp@gmail.com target=_blank><i class="fas fa-envelope-square fa-1x"></i></a></p></div></div></div></div></footer><script src=https://code.jquery.com/jquery-1.12.4.min.js></script>
<script src=https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js></script>
<script src=https://munich-nlp.github.io/js/bundle.min.8adc00f11fb328590a42ed3b14d3ec5b116abb98395a8a662d782f2150a7de6cc85eacb26c14724bb0c5130a11be2933c9b4f835cd3a053e90f036e210fbdd5d.js integrity="sha512-itwA8R+zKFkKQu07FNPsWxFqu5g5WopmLXgvIVCn3mzIXqyybBRyS7DFEwoRvikzybT4Nc06BT6Q8DbiEPvdXQ=="></script>
<script id=dsq-count-scr src=//hugo-initio-site.disqus.com/count.js async></script>
<script></script></body></html></body></html>